{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Use Delta Lake with Spark in Azure Synapse Analytics\r\n",
        "\r\n",
        "Delta Lake is an open source project to build a transactional data storage layer on top of a data lake. Delta Lake adds support for relational semantics for both batch and streaming data operations, and enables the creation of a Lakehouse architecture in which Apache Spark can be used to process and query data in tables that are based on underlying files in the data lake."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create delta tables"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%pyspark\r\n",
        "df = spark.read.load('abfss://data@asadatalake9slnoqw.dfs.core.windows.net/spark/products.csv', format='csv'\r\n",
        "## If header exists uncomment line below\r\n",
        "##, header=True\r\n",
        ")\r\n",
        "display(df.limit(10))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%pyspark\r\n",
        "df = spark.read.load('abfss://data@asadatalake9slnoqw.dfs.core.windows.net/spark/products.csv', format='csv'\r\n",
        "## If header exists uncomment line below\r\n",
        ", header=True\r\n",
        ")\r\n",
        "display(df.limit(10))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the file data into a delta table"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "delta_table_path = \"/delta/products-delta\"\r\n",
        "df.write.format(\"delta\").save(delta_table_path)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is loaded into a **deltaTable** object and updated. You can see the update reflected in the query results."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from delta.tables import *\r\n",
        "from pyspark.sql.functions import *\r\n",
        "\r\n",
        "# Create a deltaTable object\r\n",
        "deltaTable = DeltaTable.forPath(spark, delta_table_path)\r\n",
        "\r\n",
        "# Update the table (reduce price of product 771 by 10%)\r\n",
        "deltaTable.update(\r\n",
        "    condition = \"ProductID == 771\",\r\n",
        "    set = { \"ListPrice\": \"ListPrice * 0.9\" })\r\n",
        "\r\n",
        "# View the updated data as a dataframe\r\n",
        "deltaTable.toDF().show(10)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code loads the delta table data into a data frame from its location in the data lake, verifying that the change you made via a **deltaTable** object ihas been persisted."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        " new_df = spark.read.format(\"delta\").load(delta_table_path)\r\n",
        " new_df.show(10)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modify the code you just ran as follows, specifying the option to use the time travel feature of delta lake to view a previous version of the data\r\n",
        "\r\n",
        "When you run the modified code, the results show the original version of the data."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        " new_df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_table_path)\r\n",
        " new_df.show(10)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        " deltaTable.history(10).show(20, False, True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create catalog tables\r\n",
        "So far you’ve worked with delta tables by loading data from the folder containing the parquet files on which the table is based. You can define catalog tables that encapsulate the data and provide a named table entity that you can reference in SQL code. Spark supports two kinds of catalog tables for delta lake:\r\n",
        "\r\n",
        "- External tables that are defined by the path to the parquet files containing the table data.\r\n",
        "- Managed tables, that are defined in the Hive metastore for the Spark pool."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create an external table\r\n",
        "This code creates a new database named **AdventureWorks** and then creates an external tabled named **ProductsExternal** in that database based on the path to the parquet files you defined previously. It then displays a description of the table’s properties. Note that the **Location** property is the path you specified."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"CREATE DATABASE AdventureWorks\")\r\n",
        "spark.sql(\"CREATE TABLE AdventureWorks.ProductsExternal USING DELTA LOCATION '{0}'\".format(delta_table_path))\r\n",
        "spark.sql(\"DESCRIBE EXTENDED AdventureWorks.ProductsExternal\").show(truncate=False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code uses SQL to switch context to the **AdventureWorks** database (which returns no data) and then query the **ProductsExternal** table (which returns a resultset containing the products data in the Delta Lake table)."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        " %%sql\r\n",
        "\r\n",
        " USE AdventureWorks;\r\n",
        "\r\n",
        " SELECT * FROM ProductsExternal;"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a managed table\r\n",
        "This code creates a managed tabled named **ProductsManaged** based on the DataFrame you originally loaded from the **products.csv** file (before you updated the price of product 771). You do not specify a path for the parquet files used by the table - this is managed for you in the Hive metastore, and shown in the **Location** property in the table description (in the tempdata/synapse/workspaces/asaworkspacexxxxxxx/warehouse path)."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.format(\"delta\").saveAsTable(\"AdventureWorks.ProductsManaged\")\r\n",
        "spark.sql(\"DESCRIBE EXTENDED AdventureWorks.ProductsManaged\").show(truncate=False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code uses SQL to query the **ProductsManaged** table."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        " %%sql\r\n",
        "\r\n",
        " USE AdventureWorks;\r\n",
        "\r\n",
        " SELECT * FROM ProductsManaged;"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare external and managed tables\r\n",
        "This code lists the tables in the AdventureWorks database."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        " %%sql\r\n",
        "\r\n",
        " USE AdventureWorks;\r\n",
        "\r\n",
        " SHOW TABLES;"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code drops the tables from the metastore."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        " %%sql\r\n",
        "\r\n",
        " USE AdventureWorks;\r\n",
        "\r\n",
        " DROP TABLE IF EXISTS ProductsExternal;\r\n",
        " DROP TABLE IF EXISTS ProductsManaged;"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Return to the **files** tab and view the **tempdata/delta/products-delta** folder. Note that the data files still exist in this location. Dropping the external table has removed the table from the metastore, but left the data files intact.\r\n",
        "- View the **tempdata/synapse/workspaces/asaworkspacexxxxxxx/warehouse** folder, and note that there is no folder for the **ProductsManaged** table data. Dropping a managed table removes the table from the metastore and also deletes the table’s data files."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a table using SQL"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        " %%sql\r\n",
        "\r\n",
        " USE AdventureWorks;\r\n",
        "\r\n",
        " CREATE TABLE Products\r\n",
        " USING DELTA\r\n",
        " LOCATION '/delta/products-delta';"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Observe that the new catalog table was created for the existing Delta Lake table folder, which reflects the changes that were made previously."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        " %%sql\r\n",
        "\r\n",
        " USE AdventureWorks;\r\n",
        "\r\n",
        " SELECT * FROM Products;"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use delta tables for streaming data\r\n",
        "Delta lake supports streaming data. Delta tables can be a sink or a source for data streams created using the Spark Structured Streaming API. In this example, you’ll use a delta table as a sunk for some streaming data in a simulated internet of things (IoT) scenario."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        " from notebookutils import mssparkutils\r\n",
        " from pyspark.sql.types import *\r\n",
        " from pyspark.sql.functions import *\r\n",
        "\r\n",
        " # Create a folder\r\n",
        " inputPath = '/data/'\r\n",
        " mssparkutils.fs.mkdirs(inputPath)\r\n",
        "\r\n",
        " # Create a stream that reads data from the folder, using a JSON schema\r\n",
        " jsonSchema = StructType([\r\n",
        " StructField(\"device\", StringType(), False),\r\n",
        " StructField(\"status\", StringType(), False)\r\n",
        " ])\r\n",
        " iotstream = spark.readStream.schema(jsonSchema).option(\"maxFilesPerTrigger\", 1).json(inputPath)\r\n",
        "\r\n",
        " # Write some event data to the folder\r\n",
        " device_data = '''{\"device\":\"Dev1\",\"status\":\"ok\"}\r\n",
        " {\"device\":\"Dev1\",\"status\":\"ok\"}\r\n",
        " {\"device\":\"Dev1\",\"status\":\"ok\"}\r\n",
        " {\"device\":\"Dev2\",\"status\":\"error\"}\r\n",
        " {\"device\":\"Dev1\",\"status\":\"ok\"}\r\n",
        " {\"device\":\"Dev1\",\"status\":\"error\"}\r\n",
        " {\"device\":\"Dev2\",\"status\":\"ok\"}\r\n",
        " {\"device\":\"Dev2\",\"status\":\"error\"}\r\n",
        " {\"device\":\"Dev1\",\"status\":\"ok\"}'''\r\n",
        " mssparkutils.fs.put(inputPath + \"data.txt\", device_data, True)\r\n",
        " print(\"Source stream created...\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensure the message Source stream created… is printed. The code you just ran has created a streaming data source based on a folder to which some data has been saved, representing readings from hypothetical IoT devices"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Run the following code, this code writes the streaming device data in delta format."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Write the stream to a delta table\r\n",
        " delta_stream_table_path = '/delta/iotdevicedata'\r\n",
        " checkpointpath = '/delta/checkpoint'\r\n",
        " deltastream = iotstream.writeStream.format(\"delta\").option(\"checkpointLocation\", checkpointpath).start(delta_stream_table_path)\r\n",
        " print(\"Streaming to delta sink...\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Run the following code, This code reads the streamed data in delta format into a dataframe. Note that the code to load streaming data is no different to that used to load static data from a delta folder."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Read the data in delta format into a dataframe\r\n",
        " df = spark.read.format(\"delta\").load(delta_stream_table_path)\r\n",
        " display(df)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Run the following code, This code creates a catalog table named **IotDeviceData** (in the **default** database) based on the delta folder. Again, this code is the same as would be used for non-streaming data."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # create a catalog table based on the streaming sink\r\n",
        " spark.sql(\"CREATE TABLE IotDeviceData USING DELTA LOCATION '{0}'\".format(delta_stream_table_path))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code queries the IotDeviceData table, which contains the device data from the streaming source."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        " %%sql\r\n",
        "\r\n",
        " SELECT * FROM IotDeviceData;"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code writes more hypothetical device data to the streaming source."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Add more data to the source stream\r\n",
        " more_data = '''{\"device\":\"Dev1\",\"status\":\"ok\"}\r\n",
        " {\"device\":\"Dev1\",\"status\":\"ok\"}\r\n",
        " {\"device\":\"Dev1\",\"status\":\"ok\"}\r\n",
        " {\"device\":\"Dev1\",\"status\":\"ok\"}\r\n",
        " {\"device\":\"Dev1\",\"status\":\"error\"}\r\n",
        " {\"device\":\"Dev2\",\"status\":\"error\"}\r\n",
        " {\"device\":\"Dev1\",\"status\":\"ok\"}'''\r\n",
        "\r\n",
        " mssparkutils.fs.put(inputPath + \"more-data.txt\", more_data, True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code queries the **IotDeviceData** table again, which should now include the additional data that was added to the streaming source"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        " %%sql\r\n",
        "\r\n",
        " SELECT * FROM IotDeviceData;"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query a delta table from a serverless SQL pool\r\n",
        "In addition to Spark pools, Azure Synapse Analytics includes a built-in serverless SQL pool. You can use the relational database engine in this pool to query delta tables using SQL.\r\n",
        "\r\n",
        "1. In the **tempdata** tab, browse to the **tempdata/delta** folder.\r\n",
        "2. Select the **products-delta** folder, and on the toolbar, in the **New SQL script** drop-down list, select **Select TOP 100 rows**.\r\n",
        "3. In the **Select TOP 100 rows** pane, in the **File type** list, select **Delta format** and then select **Apply**.\r\n",
        "4. Review the SQL code that is generated, which should look like this:\r\n",
        "\r\n",
        "```\r\n",
        " -- This is auto-generated code\r\n",
        " SELECT\r\n",
        "     TOP 100 *\r\n",
        " FROM\r\n",
        "     OPENROWSET(\r\n",
        "         BULK 'https://datalakexxxxxxx.dfs.core.windows.net/files/delta/products-delta/',\r\n",
        "         FORMAT = 'DELTA'\r\n",
        "     ) AS [result]\r\n",
        "```\r\n",
        "5. Use the ▷ Run icon to run the script, and review the results.\r\n",
        "\r\n",
        "    This demonstrates how you can use a serverless SQL pool to query delta format files that were created using Spark, and use the results for reporting or analysis.\r\n",
        "6. Replace the query with the following SQL code:\r\n",
        "```\r\n",
        " USE AdventureWorks;\r\n",
        "\r\n",
        " SELECT * FROM Products;\r\n",
        "```\r\n",
        "7. Run the code and observe that you can also use the serverless SQL pool to query Delta Lake data in catalog tables that are defined the Spark metastore."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}